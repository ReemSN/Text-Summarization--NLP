from gensim.summarization.summarizer import summarize
import re
import nltk
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict
import string
from heapq import nlargest
import numpy as np
#from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx


text = "Santiago is a Shepherd who has a recurring dream which is supposedly prophetic. Inspired on learning this, he undertakes a journey to Egypt to discover the meaning of life and fulfill his destiny. During the course of his travels, he learns of his true purpose and meets many characters, including an “Alchemist”, that teach him valuable lessons about achieving his dreams. Santiago sets his sights on obtaining a certain kind of “treasure” for which he travels to Egypt. The key message is, “when you want something, all the universe conspires in helping you to achieve it.” Towards the final arc, Santiago gets robbed by bandits who end up revealing that the “treasure” he was looking for is buried in the place where his journey began."
sentences = sent_tokenize(text)
#load stopwords
stop = nltk.corpus.stopwords.words('english')
#remove stopwords as for text summarization purposes,
#these words add no value to word ranking.
def remove_stopwords(sentence):
    filtered_sentence = " ".join([i for i in sentence if i not in stop])
    return filtered_sentence
#Pre-process your text. Remove punctuation, special characterts, numbers, etc. As the only thing we care
#about are the actual words in the text.
clean_sentences = [s.translate(str.maketrans('','', string.punctuation)) for s in sentences]
clean_sentences = [s.translate(str.maketrans('','', string.digits)) for s in clean_sentences] #lowercase
clean_sentences = [s.lower() for s in clean_sentences]
clean_sentences = [remove_stopwords(s.split()) for s in clean_sentences]

#Weighted Word Frequencies | Word Focused
#Compute word frequencies for each sentence
word_frequencies = {}
for i in range(len(clean_sentences)):
    for word in nltk.word_tokenize(clean_sentences[i]):
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1

#Find max frequency in text and compute the weighted frequency based on the maximum frequency.
maximum_frequency = max(word_frequencies.values())
for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)
#Apply scores to each UNCLEANED SENTENCE
sentence_scores = {}
for sent in sentences:
    for word in nltk.word_tokenize(sent.lower()):
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
 
#Choose number of sentences you want in  summary
#print("$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$")
print(sentence_scores);
summary_sentences = nlargest(4, sentence_scores, key=sentence_scores.get)
#print(summary_sentences)
print("%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%")

summary = ' '.join(summary_sentences)
outF = open("b.txt", "w")
outF.write(str(summary))
outF.close()
print(summary)
